#
# .env configuration for GPU system with 6GB VRAM + 64GB RAM
# Profile: GPU-ACCELERATED HIGH
# Note: Requires Ollama to be configured for GPU usage
#

# Model selection - GPU can handle quantized large models efficiently
# Recommended: mixtral:8x7b, llama3.1:70b-q4, qwen2.5:32b, or codestral:22b
# 6GB VRAM can handle Q4 quantized models up to ~13B params, or Q8 up to ~7B
OLLAMA_MODEL=mixtral:8x7b

# Auto-pull enabled for convenience (disable for airgapped environments)
OLLAMA_AUTO_PULL=1

# Offline mode disabled (set to 1 for airgapped)
OFFLINE_STRICT=0

# Customize for your use case
USE_CASE_NAME=Domain Assistant

# Document storage path
CORPUS_PATH=/workspace/data/corpus

# Assistant behavior (optional - uses sensible default if not set)
# ASSISTANT_INSTRUCTIONS=Answer using provided context. If missing, state gaps and provide cautious best effort.

#
# Performance tuning - optimized for GPU + 64GB RAM
# GPU acceleration provides faster inference with larger models
#

# Document retrieval: 8-10 chunks (GPU can process more context efficiently)
TOP_K=8

# Context limit - GPU can handle large contexts
MAX_CONTEXT_CHARS=20000

# Model generation parameters - GPU-optimized:
# - Context window: 16384 tokens (large context possible with GPU)
# - Max response: 2048 tokens (~1500 words)
# - Threads: Lower count as GPU handles most work
# - Temperature: 0.2 (factual/deterministic)
GEN_NUM_CTX=16384
GEN_NUM_PREDICT=2048
GEN_NUM_THREAD=4
GEN_TEMPERATURE=0.2

#
# GPU NOTES:
# - Ensure NVIDIA drivers and CUDA are installed
# - Ollama will automatically detect and use GPU
# - Monitor VRAM usage with nvidia-smi
# - If model doesn't fit in 6GB VRAM, it will offload to RAM (slower but works)
# - For best performance, use Q4_0 or Q4_K_M quantized models
#
