#
# .env configuration for CPU-only system with 16GB RAM
# Profile: MEDIUM
#

# Model selection - auto will choose from: qwen2.5:3b, llama3.2:3b, mistral:7b
OLLAMA_MODEL=auto

# Auto-pull enabled for convenience (disable for airgapped environments)
OLLAMA_AUTO_PULL=1

# Offline mode disabled (set to 1 for airgapped)
OFFLINE_STRICT=0

# Customize for your use case
USE_CASE_NAME=Domain Assistant

# Document storage path
CORPUS_PATH=/workspace/data/corpus

# Assistant behavior (optional - uses sensible default if not set)
# ASSISTANT_INSTRUCTIONS=Answer using provided context. If missing, state gaps and provide cautious best effort.

#
# Performance tuning - optimized for 16GB RAM
# Leave blank to use auto-tuned values or override below
#

# Document retrieval: 4 chunks (medium profile)
TOP_K=4

# Context limit
MAX_CONTEXT_CHARS=8000

# Model generation parameters - medium profile defaults:
# - Context window: 2048 tokens
# - Max response: 384 tokens (~290 words)
# - Temperature: 0.2 (factual/deterministic)
GEN_NUM_CTX=2048
GEN_NUM_PREDICT=384
GEN_NUM_THREAD=
GEN_TEMPERATURE=0.2
