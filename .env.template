#
# .env environment variable template
#

#
# OLLAMA_MODEL
#
# When set to auto 
# - enables automatic model selection based on your system's RAM:
# - The system detects your available RAM (you have 20GB, so it
#   uses the "balanced" profile).
# - It tries to find models from a priority list already installed in Ollama
# - If none are found and auto-pull is enabled, it downloads the first model 
#   in the list
# 
# When set to a specific model (e.g. granite3.1-dense:2b):
# - The system will attempt to use that exact model
# 
# Example Models:
# - llama3.2:3b - Small Llama model (3B parameters) *preferred*
# - mistral:7b - Medium size, good balance (7B parameters)
# - llama3.1:8b - Larger Llama model (8B parameters)
# - granite3.1-dense:2b
#

OLLAMA_MODEL=auto
# OLLAMA_MODEL=llama3.2:3b

#
# OLLAMA_AUTO_PULL
#
# When set to 0 (disabled):
# - If the specified model doesn't exist in Ollama, the application will fail to start
# - You must manually download models beforehand using ollama pull <model>
# - Ideal for airgapped/offline environments where internet access is unavailable
# 
# When set to 1 (enabled):
#
# - If the specified model doesn't exist, the application will automatically download it
# - Requires internet connectivity
# - Convenient but can cause long startup delays on first run
#

OLLAMA_AUTO_PULL=1
# OLLAMA_AUTO_PULL=0

#
# OFFLINE_STRICT
#
# When set to 1 (enabled):
# - Blocks all automatic model downloads, even if OLLAMA_AUTO_PULL=1
# - Application will fail to start if the required model isn't already installed
# - Ensures zero outbound network requests for models
# - Perfect for airgapped/isolated environments with no internet access
# - Forces you to pre-install all models manually
# When set to 0 (disabled - default):
# Auto-pull behavior depends on OLLAMA_AUTO_PULL setting
# - Models can be downloaded if OLLAMA_AUTO_PULL=1
# - More flexible for internet-connected environments
#

OFFLINE_STRICT=0
# OFFLINE_STRICT=1


#
# USE_CASE_NAME is a descriptive label that customizes the AI assistant's
# identity and purpose. It is included in the system prompt sent to the AI model:
# 
# "You are a specialist assistant for this use case: {USE_CASE_NAME}."
# 
# Helps the AI understand its domain/role
#

USE_CASE_NAME=Domain Expert

#
# CORPUS_PATH specifies the directory path where your knowledge base documents
# are stored.
#
# Primary storage location for all documents you want the AI to learn from
# Documents uploaded through the UI are stored in {CORPUS_PATH}/uploads/
# Pre-existing documents can be placed directly in this directory
# When you click "Ingest Corpus", it scans this path and builds the search index
# Default: /workspace/data/corpus
#
# In the Docker container:
#
# Container path: /workspace/data/corpus
# Host path (from docker-compose.yml): data volume mount
# So documents are actually stored in corpus on your host machine
#

CORPUS_PATH=/workspace/data/corpus

#
# ASSISTANT_INSTRUCTIONS defines the behavioral guidelines and
# rules for how the AI assistant should respond to questions.
#
# Included in every prompt sent to the AI model:
#
#   "You are a specialist assistant for this use case: {USE_CASE_NAME}. 
#    Follow these instructions: {ASSISTANT_INSTRUCTIONS}"
#
# Controls how the AI handles incomplete information, uncertainty, and answer formatting
# Sets guardrails and response patterns
#
# Default (if not set):
#
#    "Answer using the provided context when possible. If context is insufficient, state what
#     is missing and provide a cautious best-effort answer."
#

# ASSISTANT_INSTRUCTIONS 

# The following are performance and behavior tuning parameters that are #automatically
# optimized based on your system's RAM. Leave them blank/commented to use \
# auto-tuned values.

#
# TOP_K
#
# Number of document chunks retrieved from the corpus to answer each question
#
# Auto-tuned by RAM:
#   ≤8GB: 3 chunks
#   ≤16GB: 4 chunks
#   ≤24GB: 4 chunks (your system)
#   24GB: 5 chunks
#
# Manual override: 
#   TOP_K=5 retrieves 5 most relevant chunks
#
# More chunks = more context but slower processing
#

TOP_K=

#
# MAX_CONTEXT_CHARS
# 
# Maximum number of characters from retrieved chunks to include in the AI prompt
# 
# Default: 
#   8000 characters
# 
# Manual override:
#   MAX_CONTEXT_CHARS=12000 for longer context
#
# Prevents prompt from becoming too long and hitting model limits

MAX_CONTEXT_CHARS=8000

#
# GEN_TEMPERATURE
#
# Controls randomness/creativity in AI responses (0.0 to 2.0)
# 
# Default: 
#   0.2 (more deterministic, factual)
#
# Values:
#   0.0 = Fully deterministic, same answer every time
#   0.2 = Slightly varied, mostly consistent (recommended for factual Q&A)
#   0.7 = Balanced creativity
#   1.5+ = Very creative, less predictable
#  
# Manual override: 
#    GEN_TEMPERATURE=0.7 for more varied responses
#

GEN_TEMPERATURE=0.2

#
# GEN_NUM_CTX
# 
# Context window size (tokens the model can "remember" in one request)
#
# Auto-tuned by RAM:
#   ≤8GB: 1536 tokens
#   ≤16GB: 2048 tokens
#   ≤24GB: 3072 tokens (your system)
#   24GB: 4096 tokens
#
# Larger = can handle longer documents/conversations but uses more RAM
#

GEN_NUM_CTX=

#
# GEN_NUM_PREDICT
#
# Maximum tokens in the AI's response (answer length limit)
# Auto-tuned by RAM:
#   8GB: 320 tokens (~240 words)
#   ≤16GB: 384 tokens (~290 words)
#   ≤24GB: 512 tokens (~380 words) (your system)
#   24GB: 640 tokens (~480 words)
#
# Manual override:
#   GEN_NUM_PREDICT=1024 for longer answers
#

GEN_NUM_PREDICT=

#
# GEN_NUM_THREAD
#
# Number of CPU threads used for inference
# 
# Auto-tuned: min(cpu_threads, 12) = uses up to 12 of your available CPU threads
# Your system: Likely 4 threads (you have 4 CPU threads)
#
# More threads = faster generation but higher CPU usage
# 
# Leave these blank unless you need specific tuning. 
# The auto-detection works well for most use cases.
#

GEN_NUM_THREAD=



