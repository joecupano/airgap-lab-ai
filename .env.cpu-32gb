#
# .env configuration for CPU-only system with 32GB RAM
# Profile: HIGH
#

# Model selection - auto will choose from: mistral:7b, qwen2.5:7b, llama3.1:8b
# Or specify a specific 7B-8B model for better performance
OLLAMA_MODEL=auto

# Auto-pull enabled for convenience (disable for airgapped environments)
OLLAMA_AUTO_PULL=1

# Offline mode disabled (set to 1 for airgapped)
OFFLINE_STRICT=0

# Customize for your use case
USE_CASE_NAME=Domain Assistant

# Document storage path
CORPUS_PATH=/workspace/data/corpus

# Assistant behavior (optional - uses sensible default if not set)
# ASSISTANT_INSTRUCTIONS=Answer using provided context. If missing, state gaps and provide cautious best effort.

#
# Performance tuning - optimized for 32GB RAM
# Leave blank to use auto-tuned values or override below
#

# Document retrieval: 5 chunks (high profile)
TOP_K=5

# Context limit - can handle more with extra RAM
MAX_CONTEXT_CHARS=12000

# Model generation parameters - high profile defaults:
# - Context window: 4096 tokens
# - Max response: 640 tokens (~480 words)
# - Temperature: 0.2 (factual/deterministic)
GEN_NUM_CTX=4096
GEN_NUM_PREDICT=640
GEN_NUM_THREAD=
GEN_TEMPERATURE=0.2
