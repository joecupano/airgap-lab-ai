#
# .env configuration for CPU-only system with 64GB RAM
# Profile: HIGH (with extended parameters)
#

# Model selection - can use larger models with abundant RAM
# Recommended: qwen2.5:14b, llama3.1:8b, or mixtral:8x7b for best quality
OLLAMA_MODEL=qwen2.5:14b

# Auto-pull enabled for convenience (disable for airgapped environments)
OLLAMA_AUTO_PULL=1

# Offline mode disabled (set to 1 for airgapped)
OFFLINE_STRICT=0

# Customize for your use case
USE_CASE_NAME=Domain Assistant

# Document storage path
CORPUS_PATH=/workspace/data/corpus

# Assistant behavior (optional - uses sensible default if not set)
# ASSISTANT_INSTRUCTIONS=Answer using provided context. If missing, state gaps and provide cautious best effort.

#
# Performance tuning - optimized for 64GB RAM
# Extended parameters for maximum quality
#

# Document retrieval: 6-8 chunks (more context with abundant RAM)
TOP_K=7

# Context limit - large context window possible
MAX_CONTEXT_CHARS=16000

# Model generation parameters - extended for 64GB:
# - Context window: 8192 tokens (2x high profile)
# - Max response: 1024 tokens (~768 words)
# - Temperature: 0.2 (factual/deterministic)
GEN_NUM_CTX=8192
GEN_NUM_PREDICT=1024
GEN_NUM_THREAD=
GEN_TEMPERATURE=0.2
